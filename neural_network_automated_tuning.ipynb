{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notbook shows how to perform k-fold cross validation and a grid search for hyperparameters.\n",
    "\n",
    "The k-fold cross validation splits up the data into k sets, and trains the neural network k times. Each time, it selects one of the k sets to be the validation sets, and trains on the other k-1 sets. After training the network k times, it reports the mean of the loss function. This process gives you a good idea of how the neural network will perform on unseen data.\n",
    "\n",
    "The grid hyperparameter search, allows you to systematically test the neural network across a range of different hyperparameters simultaneously. This is superior to tuning the parmeters individually, because many parameters are codependent. It can also be more exhaustive than manually testing hyperparameters. However, this can take a long time and may need to be run on a cluster.\n",
    "\n",
    "It is also probably a good idea to use RandomizedSearchCV instead of GridSearchCV. This is because every parameter is varied in every interation. Thus, if the network is insensitive to one of the parameters, you won't waste time varying only that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jfcre\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and convert the data to python3 type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: Run, Energy, Zen, Time delay, Q400, MuonVEM, nMuon, Type\n",
    "# Note that right now, nMuon is useless\n",
    "data = np.load('./data/NN_data.npy')\n",
    "\n",
    "# convert the python 2 bytes into python 3 format\n",
    "data_ = []\n",
    "for i in range(len(data)):\n",
    "    data_.append([])\n",
    "    for j in range(0,7):\n",
    "        data_[i].append(float(data[i,j]))\n",
    "    if data[i,7] == b'PPlus':\n",
    "        data_[i].append(1)\n",
    "    else:\n",
    "        data_[i].append(2)\n",
    "data = np.array(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data with $\\ log(E) \\in (16.0,16.5)\\ $  and  $\\ cos(zenith) > 0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = []\n",
    "for shower in data:\n",
    "    E    = shower[1]\n",
    "    logE = np.log10(E)\n",
    "    zen  = shower[2]\n",
    "    if logE >= 16 and logE <= 16.5 and np.cos(zen) > 0.9:\n",
    "        data_.append(shower)\n",
    "data_trimmed = np.array(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into input (X) and output (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_trimmed[:,1:-3]\n",
    "Y = data_trimmed[:,-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural-network model to be used in the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_base():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4,input_dim=4,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='normal'))\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell does several things:\n",
    "\n",
    "1. Sets a random seed \n",
    "2. Creates a pipeline that allows the data to be rescaled within each fold\n",
    "3. Performs k-fold cross-validation on the neural network model\n",
    "\n",
    "The output is the mean of the mean square error for the k folds, with the standard deviation in parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "\n",
    "# The Pipeline - 1. rescale data, 2. wrapper for sklearn\n",
    "scaler = ('rescale',StandardScaler())\n",
    "estimator = ('mlp',KerasRegressor(build_fn=model_base,epochs=50,batch_size=5,verbose=0))\n",
    "pipeline = Pipeline([scaler,estimator])\n",
    "\n",
    "# evaluate the model with kfold cross-validation\n",
    "n_splits = 17 # data set has len = 1462. 17 is a divisor close to 10\n",
    "kfold = KFold(n_splits=n_splits,random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline,X,Y,cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (-1*results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will grid search for different epochs and batch sizes to determine the optimum. (Note: This uses the pipeline and kfold defined in the previous cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This takes too long, so I am not going to grid search or random search right now. In principle this should be done to find the best hyperparameters. Instead I am going to make individual NN's in neural_network_manual_tuning.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "\n",
    "# The Pipeline - 1. rescale data, 2. wrapper for sklearn\n",
    "scaler = ('rescale',StandardScaler())\n",
    "estimator = ('mlp',KerasRegressor(build_fn=model_base,verbose=0))\n",
    "pipeline = Pipeline([scaler,estimator])\n",
    "\n",
    "# evaluate the model with kfold cross-validation\n",
    "n_splits = 2 # data set has len = 1462. 17 is a divisor close to 10\n",
    "kfold = KFold(n_splits=n_splits,random_state=seed)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 40, 80]\n",
    "epochs = [50, 100,150]#,200,250]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "# note I set n_jobs = -1. This uses all available cores. Set it equal to 1,2,3... to specify number of cores\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring='mean_squared_error',cv=kfold,n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
